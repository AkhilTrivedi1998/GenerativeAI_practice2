# Using Langchain with memory

**Completion models** are designed for single-turn tasks that require generating text based on a given prompt but don't necessitate maintaining a conversational history

**Chat models** are also basic completion models but because of its workflow it starts behaving differently. This workflow involves not just sending a single response each time but the entire history of exchanged messages. This is why we get the feeling of having a conversation. To execute this workflow we need **memory** to be able to store the history of exchanged messages.

**Memory** is a class that stores data, it also has access to data within the chain and can perform operations involving the two. It is accessed within the chain at two places - at the start when input is received by the chain and at the end when output is generated by the language model. At the start generally its job is to add some extra data to the input received by the chain (in case of conversational memory it will add the conversational log) and at the end it generally updates its own data with the newer exchange that has occured btw the user and the language model.

**NOTE:** For chat based models there are generally three types of messages-
1. **System message:** A message to customize how the chat bot behaves. Usually set by developers.
2. **User message:** A message created by the user
3. **AI message:** A message created by the chat model

### General workflow of chat based models

***Input --[--> Memory ---> Prompt ---> LLM ---> Memory --]--> Output***

**Placeholders** within the prompt does exactly what the name suggests, they act as placeholders and are replaced with certain values and then passed with the input data to the language model. For chat based models we can use ***MessagePlaceholder*** to create a prompt containing the entire conversational exchange.

**ChatPromptTemplate** contains two parts:
1. SystemMessagePromptTemplate: It is optional and it is used customize the model response. It contains a prompt ex - You are a chatbot specializing in {subject}
2. HumanMessagePromptTemplate: It is the human message that gets passed to the llm. It contains a prompt ex - Tell me about why I need to {query}
The input should be a dictionary containing keys of specified input variables ex - {subject: "programming", query: "write functions"}
The ChatPromptTemplate will replace the variable with the values specified by the respective keys.

**NOTE:** LangChain has many kinds of memory. For creating a chat based model we're gonna use ***ConversationBufferMemory***. It can be used to store the history of exchanged messages.

**NOTE:** We can add ***FileChatMessageHistory*** to Memory object as chat_memory attribute to create permanent storage of chat history which will keep getting updated and used everytime the program is executed.